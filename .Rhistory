<<<<<<< HEAD
body <-content (response, "text")
results <-as.data.frame(fromJSON(body))
return(results)
}
create.data.frame <- function(results) {
list <- as.data.frame(results$results.lists.books)
length <- as.numeric(length(results$results.lists.books))
info_total <- list %>% select(author, title, weeks_on_list)
for (i in 1:(length - 1)) {
i <- as.numeric(i)
author.row <- paste0('author.', i)
title.row <- paste0('title.', i)
weeks_on_list.row <- paste0('weeks_on_list.', i)
info <- list %>% select(author = author.row,
title = title.row,
weeks_on_list = weeks_on_list.row)
df <- data.frame(info)
info_total <- rbind(info_total, df)
}
info_total <- info_total %>%
distinct() %>%
group_by(author) %>%
arrange(-weeks_on_list)
return(info_total)
}
library(jsonlite)
library(dplyr)
library(httr)
weihao.api.key = 'd5996a954fbf40e788bf0a3a16e5643c'
date <- '2015-01-26'
getPublisherInfo <- function(date) {
base.url <-"https://api.nytimes.com/svc/books/v3/lists/overview.json"
query.params <- list("api-key" = weihao.api.key, "published_date" = date)
response <- GET(base.url, query = query.params)
body <-content (response, "text")
results <-as.data.frame(fromJSON(body))
list <- as.data.frame(results$results.lists.books)
length <- as.numeric(length(results$results.lists.books))
publisher.info <- list %>% select(publisher, title, weeks_on_list)
for (i in 1:(length - 1)) {
publisher.row <- paste0('publisher.', i)
title.row <- paste0('title.', i)
weeks_on_list.row <- paste0('weeks_on_list.', i)
info <- list %>% select(publisher = publisher.row,
title = title.row,
weeks_on_list = weeks_on_list.row)
df <- data.frame(info)
publisher_info <- rbind(publisher.info, df)
}
publisher_info <- publisher_info %>%
distinct() %>%
group_by(publisher) %>%
arrange(-weeks_on_list)
return(publisher_info)
}
runApp('~/Documents/INFO/FinalProject-Goodreads')
runApp('~/Documents/INFO/FinalProject-Goodreads')
View(publisher.info)
runApp('~/Documents/INFO/FinalProject-Goodreads')
runApp('~/Documents/INFO/FinalProject-Goodreads')
runApp('~/Documents/INFO/FinalProject-Goodreads')
runApp('~/Documents/INFO/FinalProject-Goodreads')
runApp('~/Documents/INFO/FinalProject-Goodreads')
runApp('~/Documents/INFO/FinalProject-Goodreads')
runApp('~/Documents/INFO/FinalProject-Goodreads')
runApp('~/Documents/INFO/FinalProject-Goodreads')
runApp('~/Documents/INFO/FinalProject-Goodreads')
runApp('~/Documents/INFO/FinalProject-Goodreads')
runApp('~/Documents/INFO/FinalProject-Goodreads')
runApp('~/Documents/INFO/FinalProject-Goodreads')
runApp('~/Documents/INFO/FinalProject-Goodreads')
runApp('~/Documents/INFO/FinalProject-Goodreads')
runApp('~/Documents/INFO/FinalProject-Goodreads')
runApp('~/Documents/INFO/FinalProject-Goodreads')
runApp('~/Documents/INFO/FinalProject-Goodreads')
runApp('~/Documents/INFO/FinalProject-Goodreads')
runApp('~/Documents/INFO/FinalProject-Goodreads')
runApp('~/Documents/INFO/FinalProject-Goodreads')
setwd("~/Documents/INFO/FinalProject-Goodreads")
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("#F8766D", "#00BFC4"),
max.words = 100)
library(reshape2)
library(wordcloud)
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("#F8766D", "#00BFC4"),
max.words = 100)
tidy_books <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
library(reshape2)
library(wordcloud)
tidy_books <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
bing_word_counts %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Contribution to sentiment",
x = NULL) +
coord_flip()
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("#F8766D", "#00BFC4"),
max.words = 100)
library(reshape2)
library(wordcloud)
library(janeaustenr)
library(dplyr)
library(stringr)
tidy_books <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
bing_word_counts %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Contribution to sentiment",
x = NULL) +
coord_flip()
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("#F8766D", "#00BFC4"),
max.words = 100)
base.url <- 'http://api.nytimes.com/svc/books/v3/reviews.json'
query.params <- list(isbn="978-0375842207", title= "The Book Thief", api_key = "ea72724367394deda8de8d2116ed19b6")
response <- GET(base.url, query = query.params)
body <- content(response, "text")
results <- fromJSON(body)
book.reviews <- flatten(results$results)
url <- book.reviews$url[1]
#more than one review - multiple word clouds -- if statement
#page <- read_html('http://www.nytimes.com/2006/05/14/books/review/14greenj.html')
#page <- read_html(book.reviews$url[1])
page <- read_html(url)
# Extract the text of review from webpage
#used the google chrome extension SelectorGadget to figure out
#which part of the html is needed
review.content <- page %>% html_nodes('.story-body-text') %>% html_text()
# Create dataframe of text
dfcontent <- data.frame(review.content, stringsAsFactors = FALSE)
# Create tidytext structure of all words in review
all.words <- dfcontent %>% unnest_tokens(word, review.content)
#Get all word counts
word.count <- all.words %>%
group_by(word) %>%
summarize(count = n()) %>%
arrange(-count)
# Remove stop words
content.words <- word.count %>%
anti_join(stop_words, by="word")
#Get content word counts -- arrange content.words by descending count
content.count <- content.words %>%
arrange(-count)
#negative + pos  words in LEXICON, not data
nrc.neg <- get_sentiments("nrc") %>%
filter(sentiment == "negative")
nrc.pos <- get_sentiments("nrc") %>%
filter(sentiment == "positive")
#counts of neg words
neg.words <- content.count %>%
inner_join(nrc.neg, by ="word")
#counts of pos words
pos.words <- content.count %>%
inner_join(nrc.pos, by ="word")
content.sent <- bind_rows(pos.words, neg.words)
cloud <- content.sent %>%
count(word, sentiment, sort =TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(scale = c(1.2,.5), random.order=TRUE, colors = c("#F8766D", "#00BFC4"),
title.size =2, max.words = 100)
library(tidytext)
library(tidytext)
library(dplyr)
library(stringr)
library(wordcloud)
library(reshape2)
libary(wordcloud)
load('wordcloud')
runApp()
library("jsonlite")
library("httr")
library("tidytext")
library("dplyr")
library("stringr")
library("rvest")
library("wordcloud")
install.packages('wordcloud')
library("wordcloud")
library("jsonlite")
library("httr")
library("tidytext")
library("dplyr")
library("stringr")
library("rvest")
library("wordcloud")
source("./api-key.R")
# test example: reviewdata <- ReviewWords("978-0375842207", "The Book Thief")
# Get all words in the book review
ReviewWords <- function(book.isbn, book.title) {
base.url <- 'http://api.nytimes.com/svc/books/v3/reviews.json'
query.params <- list(isbn = book.isbn, title = book.title,  api_key = key)
response <- GET(base.url, query = query.params)
body <- content(response, "text")
results <- fromJSON(body)
book.reviews <- flatten(results$results)
url <- book.reviews$url[1]
page <- read_html(url)
# Extract review text from webpage
review.content <- page %>% html_nodes('.story-body-text') %>% html_text()
# Create review text dataframe
dfcontent <- data.frame(review.content, stringsAsFactors = FALSE)
# Create tidytext structure of all words in review
all.words <- dfcontent %>% unnest_tokens(word, review.content)
# Get all word counts
word.count <- all.words %>%
group_by(word) %>%
summarize(count = n()) %>%
arrange(-count)
# Remove stop words
content.words <- word.count %>%
anti_join(stop_words, by="word")
# Get content word counts -- arrange content.words by descending count
content.count <- content.words %>%
arrange(-count)
return(content.count)
}
reviewdata <- ReviewWords <- function(book.isbn, book.title)
# Get wordcloud with negative words in review
NegativeCloud <- function(reviewdata) {
# Get negative words in "nrc" Lexicon
nrc.neg <- get_sentiments("nrc") %>%
filter(sentiment == "negative")
# Count the negative words in the review data
neg.words <- reviewdata %>%
inner_join(nrc.neg, by ="word")
# Create the negative word cloud
ncloud <- wordcloud(neg.words$word, neg.words$count, scale = c(3,.5), min.freq = 1, max.words = Inf, random.order = TRUE, colors = "#C40500")
return(ncloud)
}
negative.cloud <- NegativeCloud(reviewdata)
# Get wordcloud with positive words in review
PositiveCloud <- function(content.count) {
# Get positive words in "nrc" Lexicon
nrc.pos <- get_sentiments("nrc") %>%
filter(sentiment == "positive")
# Count the positive words in the review data
pos.words <- content.count %>%
inner_join(nrc.pos, by ="word")
# Create the positive word cloud
pcloud <- wordcloud(pos.words$word, pos.words$count, scale = c(3,.5), min.freq = 1, max.words = Inf,
random.order = TRUE, colors = "#00BFC4")
return(pcloud)
}
positive.cloud <- PositiveCloud(reviewdata)
#this source: http://tidytextmining.com/tidytext.html
#provided helpful information for working with tidytext data and
#for text mining in R
source("api.key.R")
# test example: reviewdata <- ReviewWords("978-0375842207", "The Book Thief")
# Get all words in the book review
ReviewWords <- function(book.isbn, book.title) {
base.url <- 'http://api.nytimes.com/svc/books/v3/reviews.json'
query.params <- list(isbn = book.isbn, title = book.title,  api_key = key)
response <- GET(base.url, query = query.params)
body <- content(response, "text")
results <- fromJSON(body)
book.reviews <- flatten(results$results)
url <- book.reviews$url[1]
page <- read_html(url)
# Extract review text from webpage
review.content <- page %>% html_nodes('.story-body-text') %>% html_text()
# Create review text dataframe
dfcontent <- data.frame(review.content, stringsAsFactors = FALSE)
# Create tidytext structure of all words in review
all.words <- dfcontent %>% unnest_tokens(word, review.content)
# Get all word counts
word.count <- all.words %>%
group_by(word) %>%
summarize(count = n()) %>%
arrange(-count)
# Remove stop words
content.words <- word.count %>%
anti_join(stop_words, by="word")
# Get content word counts -- arrange content.words by descending count
content.count <- content.words %>%
arrange(-count)
return(content.count)
}
reviewdata <- ReviewWords <- function(book.isbn, book.title)
# Get wordcloud with negative words in review
NegativeCloud <- function(reviewdata) {
# Get negative words in "nrc" Lexicon
nrc.neg <- get_sentiments("nrc") %>%
filter(sentiment == "negative")
# Count the negative words in the review data
neg.words <- reviewdata %>%
inner_join(nrc.neg, by ="word")
# Create the negative word cloud
ncloud <- wordcloud(neg.words$word, neg.words$count, scale = c(3,.5), min.freq = 1, max.words = Inf, random.order = TRUE, colors = "#C40500")
return(ncloud)
}
negative.cloud <- NegativeCloud(reviewdata)
# Get wordcloud with positive words in review
PositiveCloud <- function(content.count) {
# Get positive words in "nrc" Lexicon
nrc.pos <- get_sentiments("nrc") %>%
filter(sentiment == "positive")
# Count the positive words in the review data
pos.words <- content.count %>%
inner_join(nrc.pos, by ="word")
# Create the positive word cloud
pcloud <- wordcloud(pos.words$word, pos.words$count, scale = c(3,.5), min.freq = 1, max.words = Inf,
random.order = TRUE, colors = "#00BFC4")
return(pcloud)
}
positive.cloud <- PositiveCloud(reviewdata)
#this source: http://tidytextmining.com/tidytext.html
#provided helpful information for working with tidytext data and
#for text mining in R
source("api.key.R")
# test example: reviewdata <- ReviewWords("978-0375842207", "The Book Thief")
book.isbn <- "978-0375842207"
book.title <- "The Book Thief"
base.url <- 'http://api.nytimes.com/svc/books/v3/reviews.json'
query.params <- list(isbn = book.isbn, title = book.title,  api_key = key)
response <- GET(base.url, query = query.params)
body <- content(response, "text")
results <- fromJSON(body)
book.reviews <- flatten(results$results)
url <- book.reviews$url[1]
page <- read_html(url)
book.isbn <- "978-0375842207"
book.title <- "The Book Thief"
base.url <- 'http://api.nytimes.com/svc/books/v3/reviews.json'
query.params <- list(isbn = book.isbn, title = book.title,  api_key = key)
response <- GET(base.url, query = query.params)
body <- content(response, "text")
results <- fromJSON(body)
book.reviews <- flatten(results$results)
url <- book.reviews$url[1]
page <- read_html(url)
# Extract review text from webpage
review.content <- page %>% html_nodes('.story-body-text') %>% html_text()
# Create review text dataframe
dfcontent <- data.frame(review.content, stringsAsFactors = FALSE)
# Create tidytext structure of all words in review
all.words <- dfcontent %>% unnest_tokens(word, review.content)
# Get all word counts
word.count <- all.words %>%
group_by(word) %>%
summarize(count = n()) %>%
arrange(-count)
book.isbn <- "978-0375842207"
book.title <- "The Book Thief"
base.url <- 'http://api.nytimes.com/svc/books/v3/reviews.json'
query.params <- list(isbn = book.isbn, title = book.title,  api_key = key)
response <- GET(base.url, query = query.params)
body <- content(response, "text")
results <- fromJSON(body)
book.reviews <- flatten(results$results)
url <- book.reviews$url[1]
page <- read_html(url)
# Extract review text from webpage
review.content <- page %>% html_nodes('.story-body-text') %>% html_text()
# Create review text dataframe
dfcontent <- data.frame(review.content, stringsAsFactors = FALSE)
# Create tidytext structure of all words in review
all.words <- dfcontent %>% unnest_tokens(word, review.content)
# Get all word counts
word.count <- all.words %>%
group_by(word) %>%
summarize(count = n()) %>%
arrange(-count)
# Extract review text from webpage
review.content <- page %>% html_nodes('.story-body-text') %>% html_text()
# Create review text dataframe
dfcontent <- data.frame(review.content, stringsAsFactors = FALSE)
# Create tidytext structure of all words in review
all.words <- dfcontent %>% unnest_tokens(word, review.content)
# Get all word counts
word.count <- all.words %>%
group_by(word) %>%
summarize(count = n()) %>%
arrange(-count)
book.isbn <- "978-0375842207"
book.title <- "The Book Thief"
base.url <- 'http://api.nytimes.com/svc/books/v3/reviews.json'
query.params <- list(isbn = book.isbn, title = book.title,  api_key = key)
response <- GET(base.url, query = query.params)
body <- content(response, "text")
results <- fromJSON(body)
book.reviews <- flatten(results$results)
url <- book.reviews$url[1]
page <- read_html(url)
# Extract review text from webpage
review.content <- page %>% html_nodes('.story-body-text') %>% html_text()
# Create review text dataframe
dfcontent <- data.frame(review.content, stringsAsFactors = FALSE)
book.isbn <- "978-0375842207"
book.title <- "The Book Thief"
base.url <- 'http://api.nytimes.com/svc/books/v3/reviews.json'
query.params <- list(isbn = book.isbn, title = book.title,  api_key = key)
response <- GET(base.url, query = query.params)
body <- content(response, "text")
results <- fromJSON(body)
book.reviews <- flatten(results$results)
url <- book.reviews$url[1]
page <- read_html(url)
book.isbn <- "978-0375842207"
book.title <- "The Book Thief"
base.url <- 'http://api.nytimes.com/svc/books/v3/reviews.json'
query.params <- list(isbn = book.isbn, title = book.title,  api_key = key)
response <- GET(base.url, query = query.params)
body <- content(response, "text")
results <- fromJSON(body)
source("api.key.R")
# test example: reviewdata <- ReviewWords("978-0375842207", "The Book Thief")
book.isbn <- "978-0375842207"
book.title <- "The Book Thief"
base.url <- 'http://api.nytimes.com/svc/books/v3/reviews.json'
query.params <- list(isbn = book.isbn, title = book.title,  api_key = key)
response <- GET(base.url, query = query.params)
body <- content(response, "text")
results <- fromJSON(body)
response <- GET(base.url, query = query.params)
query.params <- list(isbn = book.isbn, title = book.title,  api_key = key)
source("api.key.R")
# test example: reviewdata <- ReviewWords("978-0375842207", "The Book Thief")
book.isbn <- "978-0375842207"
book.title <- "The Book Thief"
base.url <- 'http://api.nytimes.com/svc/books/v3/reviews.json'
query.params <- list(isbn = book.isbn, title = book.title,  api_key = key)
response <- GET(base.url, query = query.params)
body <- content(response, "text")
results <- fromJSON(body)
book.reviews <- flatten(results$results)
url <- book.reviews$url[1]
page <- read_html(url)
review.content <- page %>% html_nodes('.story-body-text') %>% html_text()
dfcontent <- data.frame(review.content, stringsAsFactors = FALSE)
View(dfcontent)
all.words <- dfcontent %>% unnest_tokens(word, review.content)
# Get all word counts
word.count <- all.words %>%
group_by(word) %>%
summarize(count = n()) %>%
arrange(-count)
# Remove stop words
content.words <- word.count %>%
anti_join(stop_words, by="word")
View(content.words)
runApp()
setwd("~/Documents/INFO/FinalProject-Goodreads")
=======
fluidRow(
splitLayout(cellWidths = 300, plotOutput("negplot"), plotOutput("posplot"))
)
)
)
shinyApp(ui = ui, server = server)
server <- function(input, output) {
output$negplot <- renderPlot({
reviewdata <- ReviewWords(input$isbn, input$title)
negative.cloud <- NegativeCloud(reviewdata)
})
output$posplot <- renderPlot({
reviewdata <- ReviewWords(input$isbn, input$title)
positive.cloud <- PositiveCloud(reviewdata)
=======
#Combined all price for all the book in my dataframe. Now the data frame has "Book Title","Price","# of weeks the book has been on the Bestseller List", and "Publisher".
all.price <-do.call(rbind,as.list(PriceInfo))
combined.df <-combined.df %>% select(-buy_links)
combined.df[,"price"]=as.vector(all.price)
combined.df <-combined.df %>% filter(price!="NA")
# Generate a plot with all four column's information by Plotly.
f1 <- list(
family = "Arial, sans-serif",
size = 18,
color = "#FBBBB9"
)
f2 <- list(
family = "Old Standard TT, serif",
size = 14,
color = "black"
)
a <- list(
title = "Price",
titlefont = f1,
showticklabels = TRUE,
tickfont = f2,
exponentformat = "E"
)
b <- list(
title = "Popularity",
titlefont = f1,
showticklabels = TRUE,
tickfont = f2,
exponentformat = "E"
)
yiranpriceplot <- plot_ly(
combined.df, x = ~price, y = ~weeks_on_list,type="scatter",mode = 'markers',marker=list(opacity =0.5,color= "#43C6DB"),
text = ~paste(paste("Book Title:", title),
paste("Publisher:", publisher),
paste("Popularity:",weeks_on_list),
paste("Price: $", price),
sep="<br />")) %>%
layout(xaxis = a, yaxis = b)
return(yiranpriceplot)
}
# Ui
source("price copy.R")
shinyUI(fluidPage(
titlePanel("Yiran's Price Chart"),
sidebarLayout(
sidebarPanel(
dateRangeInput("dates", label = h3("Date range")),
hr(),
fluidRow(column(4, verbatimTextOutput("value")))
),
mainPanel(
plotlyOutput("yiranPlot")
)
)
))
#Server
source("price copy.R")
# Define server logic required to draw a histogram
shinyServer(function(input, output) {
output$yiranPlot <-renderPlotly({
UserCombinedInfo(input$dates[1],input$dates[2])
})
})
take a really long time, and the NYT API rate limit, which is 1000 calls/dates, might exceed until you contact NYT API team.
# It would take about 10-15min to generate a graph for one input date. While the optimal goal of my function is returning a graph no matter how long the time period the user requests,I suggest TAs input two same dates (2017-06-08 to 2017-06-08) on Shiny web when you test my code, so that it will take much less time. Please be patient, there will be a graph generated after 15min or so on Shiny web page. Thanks!
UserCombinedInfo <- function(startdate,enddate) {
# set the time period. The startdate and enddate are the user's inputs
time.period <-seq(as.Date(startdate), as.Date(enddate), by=1)
# create a data frame with the information NYT Book API "List/Overview" returns to me.
OneDateInfo <- function (date) {
base.url <-"https://api.nytimes.com/svc/books/v3/lists/overview.json"
query.params <-list("api-key"="cb7ad64f78f24f90a6b29ccb6576821a", "published_date"=date)
response <- GET(base.url, query = query.params)
body <-content (response, "text")
results <-as.data.frame(fromJSON(body))
UsefulInfo <- by(results,1:nrow(results),function(row){
each <-as.data.frame(row$results.lists.books)
return(each)
})
all.useful <-do.call(rbind,UsefulInfo)
}
#Combined every date's info into one data frame.
combined.df.all <-lapply(time.period,OneDateInfo)
combined.df.all <-as.data.frame(do.call(rbind,combined.df.all)) %>%
filter(weeks_on_list!=0) %>%
filter(weeks_on_list > 10)
combined.df <-unique(combined.df.all) %>%
select(title, weeks_on_list, buy_links,publisher)
row.names(combined.df) <-NULL
#Since the NYT's price column is always 0, I decided to request price through the Barnes&Noble Online Bookshop URL, which is given by NYT.
PriceInfo <-by(combined.df,1:nrow(combined.df),function(newrow){
every <-as.data.frame(newrow$buy_links)
price.url <-every[3,2]
page <-read_html(GET(price.url,user_agent("myagnet")))
price1 <- page %>% html_nodes('#pdp-cur-price') %>% html_text()
if (length(price1)==1) {
price2 <-as.numeric(sub('\\$','',as.character(price1)))
} else if (length(price1)==0) {
price2 <- page %>% html_nodes('#pdp-cur-price-BuyNew') %>% html_text()
price2 <-as.numeric(sub('\\$','',as.character(price2)))
}
if (length(price2)==0) {
price2=NA
}
return(price2)
})
#Combined all price for all the book in my dataframe. Now the data frame has "Book Title","Price","# of weeks the book has been on the Bestseller List", and "Publisher".
all.price <-do.call(rbind,as.list(PriceInfo))
combined.df <-combined.df %>% select(-buy_links)
combined.df[,"price"]=as.vector(all.price)
combined.df <-combined.df %>% filter(price!="NA")
# Generate a plot with all four column's information by Plotly.
f1 <- list(
family = "Arial, sans-serif",
size = 18,
color = "#FBBBB9"
)
f2 <- list(
family = "Old Standard TT, serif",
size = 14,
color = "black"
)
a <- list(
title = "Price",
titlefont = f1,
showticklabels = TRUE,
tickfont = f2,
exponentformat = "E"
)
b <- list(
title = "Popularity",
titlefont = f1,
showticklabels = TRUE,
tickfont = f2,
exponentformat = "E"
)
yiranpriceplot <- plot_ly(
combined.df, x = ~price, y = ~weeks_on_list,type="scatter",mode = 'markers',marker=list(opacity =0.5,color= "#43C6DB"),
text = ~paste(paste("Book Title:", title),
paste("Publisher:", publisher),
paste("Popularity:",weeks_on_list),
paste("Price: $", price),
sep="<br />")) %>%
layout(xaxis = a, yaxis = b)
return(yiranpriceplot)
}
gh <-UserCombinedInfo("2017-09-09","2017-09-09")
base.url <-"https://api.nytimes.com/svc/books/v3/lists/overview.json"
query.params <-list("api-key"="cb7ad64f78f24f90a6b29ccb6576821a", "published_date"="2017-01-01")
response <- GET(base.url, query = query.params)
body <-content (response, "text")
results <-as.data.frame(fromJSON(body))
View(results)
UserCombinedInfo <- function(startdate,enddate) {
# set the time period. The startdate and enddate are the user's inputs
time.period <-seq(as.Date(startdate), as.Date(enddate), by=1)
# create a data frame with the information NYT Book API "List/Overview" returns to me.
OneDateInfo <- function (date) {
base.url <-"https://api.nytimes.com/svc/books/v3/lists/overview.json"
query.params <-list("api-key"=yiran.api.key, "published_date"=date)
response <- GET(base.url, query = query.params)
body <-content (response, "text")
results <-as.data.frame(fromJSON(body))
UsefulInfo <- by(results,1:nrow(results),function(row){
each <-as.data.frame(row$results.lists.books)
return(each)
>>>>>>> fb174541bf330c7211b900d46d64d6692753e94a
})
}
<<<<<<< HEAD
ui <- fluidPage(
headerPanel("Reviews"),
sidebarPanel(
textInput("title", label = h3("Title"), value = "The Book Thief"),
textInput("isbn", label = h3("ISBN"), value = "978-0375842207"),
hr(),
fluidRow(column(3, verbatimTextOutput("value")))
),
mainPanel(
fluidRow(
splitLayout(cellWidths = 300, plotOutput("negplot"), plotOutput("posplot"))
)
)
)
shinyApp(ui = ui, server = server)
server <- function(input, output) {
output$negplot <- renderPlot({
reviewdata <- ReviewWords(input$isbn, input$title)
negative.cloud <- NegativeCloud(reviewdata)
})
output$posplot <- renderPlot({
reviewdata <- ReviewWords(input$isbn, input$title)
positive.cloud <- PositiveCloud(reviewdata)
})
=======
#Combined every date's info into one data frame.
combined.df.all <-lapply(time.period,OneDateInfo)
combined.df.all <-as.data.frame(do.call(rbind,combined.df.all)) %>%
filter(weeks_on_list!=0) %>%
filter(weeks_on_list > 10)
combined.df <-unique(combined.df.all) %>%
select(title, weeks_on_list, buy_links,publisher)
row.names(combined.df) <-NULL
#Since the NYT's price column is always 0, I decided to request price through the Barnes&Noble Online Bookshop URL, which is given by NYT.
PriceInfo <-by(combined.df,1:nrow(combined.df),function(newrow){
every <-as.data.frame(newrow$buy_links)
price.url <-every[3,2]
page <-read_html(GET(price.url,user_agent("myagnet")))
price1 <- page %>% html_nodes('#pdp-cur-price') %>% html_text()
if (length(price1)==1) {
price2 <-as.numeric(sub('\\$','',as.character(price1)))
} else if (length(price1)==0) {
price2 <- page %>% html_nodes('#pdp-cur-price-BuyNew') %>% html_text()
price2 <-as.numeric(sub('\\$','',as.character(price2)))
>>>>>>> fb174541bf330c7211b900d46d64d6692753e94a
}
ui <- fluidPage(
headerPanel("Reviews"),
sidebarPanel(
textInput("title", label = h3("Title"), value = "The Book Thief"),
textInput("isbn", label = h3("ISBN"), value = "978-0375842207"),
hr(),
fluidRow(column(3, verbatimTextOutput("value")))
),
mainPanel(
fluidRow(
splitLayout(plotOutput("negplot"), plotOutput("posplot"))
)
)
)
shinyApp(ui = ui, server = server)
server <- function(input, output) {
output$negplot <- renderPlot({
par(mar=c(0,6,2,2)+0.1)
reviewdata <- ReviewWords(input$isbn, input$title)
negative.cloud <- NegativeCloud(reviewdata)
})
output$posplot <- renderPlot({
par(mar=c(0,6,2,2)+0.1)
reviewdata <- ReviewWords(input$isbn, input$title)
positive.cloud <- PositiveCloud(reviewdata)
})
}
<<<<<<< HEAD
ui <- fluidPage(
headerPanel("Reviews"),
sidebarPanel(
textInput("title", label = h3("Title"), value = "The Book Thief"),
textInput("isbn", label = h3("ISBN"), value = "978-0375842207"),
hr(),
fluidRow(column(3, verbatimTextOutput("value")))
),
mainPanel(
fluidRow(
splitLayout(plotOutput("negplot"), plotOutput("posplot"))
)
)
)
shinyApp(ui = ui, server = server)
server <- function(input, output) {
output$negplot <- renderPlot({
par(mar=c(0,6,2,2)+0.1)
reviewdata <- ReviewWords(input$isbn, input$title)
negative.cloud <- NegativeCloud(reviewdata)
})
output$posplot <- renderPlot({
par(mar=c(0,6,2,2)+0.1)
reviewdata <- ReviewWords(input$isbn, input$title)
positive.cloud <- PositiveCloud(reviewdata)
})
}
ui <- fluidPage(
headerPanel("Reviews"),
sidebarPanel(
textInput("title", label = h3("Title"), value = "The Book Thief"),
textInput("isbn", label = h3("ISBN"), value = "978-0375842207"),
hr(),
fluidRow(column(3, verbatimTextOutput("value")))
),
mainPanel(
fluidRow(
splitLayout(plotOutput("negplot"), plotOutput("posplot"))
)
)
)
shinyApp(ui = ui, server = server)
=======
return(price2)
})
#Combined all price for all the book in my dataframe. Now the data frame has "Book Title","Price","# of weeks the book has been on the Bestseller List", and "Publisher".
all.price <-do.call(rbind,as.list(PriceInfo))
combined.df <-combined.df %>% select(-buy_links)
combined.df[,"price"]=as.vector(all.price)
combined.df <-combined.df %>% filter(price!="NA")
# Generate a plot with all four column's information by Plotly.
f1 <- list(
family = "Arial, sans-serif",
size = 18,
color = "#FBBBB9"
)
f2 <- list(
family = "Old Standard TT, serif",
size = 14,
color = "black"
)
a <- list(
title = "Price",
titlefont = f1,
showticklabels = TRUE,
tickfont = f2,
exponentformat = "E"
)
b <- list(
title = "Popularity",
titlefont = f1,
showticklabels = TRUE,
tickfont = f2,
exponentformat = "E"
)
yiranpriceplot <- plot_ly(
combined.df, x = ~price, y = ~weeks_on_list,type="scatter",mode = 'markers',marker=list(opacity =0.5,color= "#43C6DB"),
text = ~paste(paste("Book Title:", title),
paste("Publisher:", publisher),
paste("Popularity:",weeks_on_list),
paste("Price: $", price),
sep="<br />")) %>%
layout(xaxis = a, yaxis = b)
return(yiranpriceplot)
}
runApp()
j <-Sys.Date()
j
runApp()
View(combined.df)
View(results)
View(combined.df)
View(combined.df)
time.period <-seq(as.Date("2016-09-09"), as.Date("2016-09-09"), by=1)
# create a data frame with the information NYT Book API "List/Overview" returns to me.
OneDateInfo <- function (date) {
base.url <-"https://api.nytimes.com/svc/books/v3/lists/overview.json"
query.params <-list("api-key"=yiran.api.key, "published_date"=date)
response <- GET(base.url, query = query.params)
body <-content (response, "text")
results <-as.data.frame(fromJSON(body))
UsefulInfo <- by(results,1:nrow(results),function(row){
each <-as.data.frame(row$results.lists.books)
return(each)
})
all.useful <-do.call(rbind,UsefulInfo)
}
#Combined every date's info into one data frame.
combined.df.all <-lapply(time.period,OneDateInfo)
combined.df.all <-as.data.frame(do.call(rbind,combined.df.all)) %>%
# Requesting price of each book will take a LONG time. Normally there will be 210 rows in the origin data frame #(42lists*5). I filter the data frame so that only the books whose weeks_on_list are over than 10 will show up in the new data frame. Therefore we are requesting the price information from the 1
filter(weeks_on_list!=0) %>%
filter(weeks_on_list > 10)
View(combined.df)
View(combined.df.all)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
>>>>>>> fb174541bf330c7211b900d46d64d6692753e94a
# Import essential libraries
library(httr)
library(jsonlite)
library(dplyr)
library(shiny)
library(plotly)
library(tidyr)
source('api.key.R')
# Get the name of all lists
list.names.url <- 'https://api.nytimes.com/svc/books/v3/lists/names.json'
list.names.query.params <- list(api_key = book.key)
list.names.response <- GET(list.names.url, query = list.names.query.params)
list.names.body <- content(list.names.response, "text")
list.names.results <- fromJSON(list.names.body)$results
list.names.display <- list.names.results$display_name
shinyUI(popularity.ui <- fluidPage({
sidebarLayout(
sidebarPanel(
selectInput("pop.indicator", label = h3("Popularity Indicator"),
choices = list("Rank" = "Rank", "Weeks on List" = "Week")),
htmlOutput('selectUI'),
htmlOutput('dateUI')
),
mainPanel(
plotlyOutput('popularity')
)
)
}))
# All updates kept in the server
shinyServer(popularity.server <- function(input, output){
output$selectUI <- renderUI({
selectInput("list.select", label = h3("Select A Bestseller List"),
choices = list.names.display, selected = 1)
})
output$dateUI <- renderUI({
list.selected <- list.names.results[list.names.results$display_name == input$list.select, ]
first.date <- list.selected[['oldest_published_date']]
last.date <- list.selected[['newest_published_date']]
dateRangeInput("list.range", label = h3("Select Publish Date Range"),
start = '2017-01-01', end = '2017-02-01',
min = first.date, max = last.date, separator = " ")
})
output$popularity <- renderPlotly({
list.selected <- list.names.results[list.names.results$display_name == input$list.select, ]
date.from <- strsplit(toString(input$list.range), split = " ")[[1]][1]
date.to <- strsplit(toString(input$list.range), split = " ")[[1]][2]
print(date.to)
list.dates <- seq(as.Date(date.from), as.Date(date.to), by = 'week')
UserCombinedInfo <- function(time.period) {
one_date <- function(date){
base.url <- 'https://api.nytimes.com/svc/books/v3/lists/overview.json'
query.params <-list("api-key"= book.key, "published_date"=date)
response <- GET(base.url, query = query.params)
body <-content (response, "text")
results <- as.data.frame(fromJSON(body)$results) %>%
filter(lists.list_name_encoded == list.selected$list_name_encoded)}
combined.df <- lapply(time.period, one_date)
combined.df <- as.data.frame(do.call(rbind,combined.df)) %>% unique()
combined.df <- unnest(combined.df, lists.books)
return(combined.df)
}
all.lists <- UserCombinedInfo(list.dates)
all.lists$date <- as.Date(all.lists$created_date)
if(input$pop.indicator == "Rank") {
p <- plot_ly(all.lists, x = ~date, y = ~rank, color = ~title) %>%
layout(yaxis = list(title = "Rank"), xaxis = list(title = "Date"))
} else {
p <- plot_ly(all.lists, x = ~date, y = ~weeks_on_list, color = ~title) %>%
layout(yaxis = list(title = "Weeks on List"), xaxis = list(title = "Date"))
}
p <- p %>%
add_lines() %>%
layout(legend = list(font = list(
family = "sans-serif",
size = 12,
color = "#000"),
bgcolor = "#E2E2E2",
bordercolor = "#FFFFFF",
borderwidth = 2)) %>%
layout(autosize = F, width = 1000, height = 500, margin = list(
l = 50,
r = 50,
b = 100,
t = 100,
pad = 4
))
return(p)
})
})
runApp('popularity.R')
runApp('popularity.R')
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
setwd('../FinalProject-Goodreads/')
runApp()
runApp()
runApp()
runApp('C:/Users/ytma9/Desktop')
runApp()
runApp()
shiny::runApp()
library(rvest)
install.packages('rvest')
install.packages('curl')
install.packages('ggplot2')
install.packages("ggplot2")
install.packages('tidytext')
install.packages('stringr')
install.packages('wordcloud')
install.packages('reshape')
install.packages('reshape2')
runApp()
runApp()
>>>>>>> 9006147b9eaebc24a9186e3ba01ddbd62ed6dbce
runApp()
runApp()
runApp()
runApp()
<<<<<<< HEAD
query.params <- list("api-key" = key, "published_date" = date)
runApp()
runApp('~/Documents/INFO/Final Project')
runApp('~/Documents/INFO/Final Project')
=======
shiny::runApp()
runApp()
>>>>>>> 9006147b9eaebc24a9186e3ba01ddbd62ed6dbce
